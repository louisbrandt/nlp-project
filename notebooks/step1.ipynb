{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_save = True\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pickle\n",
    "\n",
    "import fugashi # pip install fugashi or pip install fugashi[unidic-lite]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data from previously saved file\n",
    "with open(f'../data/amazon_reviews/train/data_train.pickle', 'rb') as train_file:\n",
    "    train_prepped_data = pickle.load(train_file)\n",
    "with open(f'../data/amazon_reviews/test/data_test.pickle', 'rb') as file:\n",
    "    test_prepped_data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_encode(sent):\n",
    "    '''\n",
    "    Helper function to encode sentiment\n",
    "    ------\n",
    "    Takes in string description\n",
    "        'sent' - either positive or negative\n",
    "    Returns binary encoding\n",
    "        1 = positive sentiment\n",
    "        0 = negative sentiment\n",
    "    '''\n",
    "    if float(sent) > 2.5:\n",
    "        return 1 # positive\n",
    "    else:\n",
    "        return 0 # negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(data, language):\n",
    "    '''\n",
    "    Function to clean the data\n",
    "    -----\n",
    "    Takes in data set from load_data() and language needed to tokenize in\n",
    "        'data' - nested dictionary  \n",
    "    Returns two lists\n",
    "        cleaned - X list of tuples (id,[text])\n",
    "        ys - y list\n",
    "    '''\n",
    "    cleaned = [] \n",
    "    ys = []\n",
    "    tagger = fugashi.Tagger()\n",
    "\n",
    "    for idx in data:\n",
    "        review = data[idx].get('text', None) # some data does not have a review text\n",
    "        summary = data[idx].get('summary', None) # some data does not have a summary \n",
    "        \n",
    "        # combine summary and review\n",
    "        if review == None and summary == None:\n",
    "            text = ''\n",
    "        elif review == None:\n",
    "            text = summary\n",
    "        elif summary == None:\n",
    "            text = review\n",
    "        else:\n",
    "            text = summary + ' ' + review\n",
    "        \n",
    "        # tokenizing for english and french reviews\n",
    "        if language == \"english\" or language == \"french\":\n",
    "            text = text.lower()\n",
    "            sequence = word_tokenize(text, language=language)  # splits gotta into got ta\n",
    "            cleaned.append(sequence)\n",
    "            # encode sentiment\n",
    "            ys.append(sent_encode(data[idx]['rating']))\n",
    "\n",
    "        # tokenizing for japanese reviews\n",
    "        elif language == \"japanese\":\n",
    "            text = text.lower()\n",
    "            sequence = [word.surface for word in tagger(text)]\n",
    "            cleaned.append(sequence)\n",
    "            # encode sentiment\n",
    "            ys.append(sent_encode(data[idx]['rating']))\n",
    "\n",
    "    return cleaned, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_corpus(dataset):\n",
    "    '''\n",
    "    Function computing vocabluary and corpus for a dataset\n",
    "    -----\n",
    "    Takes a cleaned dataset - list \n",
    "        dataset - X list \n",
    "    Returns\n",
    "        vocab - set of unique tokens in dataset\n",
    "        corpus - list of strings; sentences in dataset \n",
    "    '''\n",
    "    vocab = set()\n",
    "    corpus = []\n",
    "    for text in dataset: # for list in list of lists\n",
    "        sentence = ''\n",
    "        for token in text: # for token in list \n",
    "            vocab.add(token)\n",
    "            if token in ['.','!','?',',',';',':']:\n",
    "                sentence += token \n",
    "            else:\n",
    "                sentence += ' ' + token \n",
    "        corpus.append(sentence.lstrip()) \n",
    "    return vocab, corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_en, y_train_en = clean(train_prepped_data['en'], \"english\") # english tokenized text\n",
    "train_fr, y_train_fr = clean(train_prepped_data['fr'], \"french\") # french tokenized text\n",
    "train_jp, y_train_jp = clean(train_prepped_data['jp'], \"japanese\") # japanese tokenized text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_en, y_test_en = clean(test_prepped_data['en'], \"english\") # english tokenized text\n",
    "test_fr, y_test_fr = clean(test_prepped_data['fr'], \"french\") # french tokenized text\n",
    "test_jp, y_test_jp = clean(test_prepped_data['jp'], \"japanese\") # japanese tokenized text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview of positive/negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_sent(y):\n",
    "    positive = 0\n",
    "    negative = 0\n",
    "    for i in y:\n",
    "        if i == 1:\n",
    "            positive += 1\n",
    "        elif i == 0:\n",
    "            negative += 1 \n",
    "    return positive,negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos,neg = count_sent(y_train_en)\n",
    "\n",
    "print(\"Number of positive reviews (English): \", pos)\n",
    "print(\"Number of negative reviews (English): \", neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos,neg = count_sent(y_train_fr)\n",
    "\n",
    "print(\"Number of positive reviews (French): \", pos)\n",
    "print(\"Number of negative reviews (French): \", neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos,neg = count_sent(y_train_jp)\n",
    "\n",
    "print(\"Number of positive reviews (Japanese): \", pos)\n",
    "print(\"Number of negative reviews (Japanese): \", neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocab & Corpus\n",
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocabulary_en, train_corpus_en = get_vocab_corpus(train_en)\n",
    "train_vocabulary_fr, train_corpus_fr = get_vocab_corpus(train_fr)\n",
    "train_vocabulary_jp, train_corpus_jp = get_vocab_corpus(train_jp)\n",
    "\n",
    "print(\"Vocab size of {} is: {} and Corpus size is: {}\".format('English', len(train_vocabulary_en), len(train_corpus_en)))\n",
    "print(\"Vocab size of {} is: {} and Corpus size is: {}\".format('French', len(train_vocabulary_fr), len(train_corpus_fr)))\n",
    "print(\"Vocab size of {} is: {} and Corpus size is: {}\".format('Japanese', len(train_vocabulary_jp), len(train_corpus_jp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vocabulary_en, test_corpus_en = get_vocab_corpus(test_en)\n",
    "test_vocabulary_fr, test_corpus_fr = get_vocab_corpus(test_fr)\n",
    "test_vocabulary_jp, test_corpus_jp = get_vocab_corpus(test_jp)\n",
    "\n",
    "print(\"Vocab size of {} is: {} and Corpus size is: {}\".format('English', len(test_vocabulary_en), len(test_corpus_en)))\n",
    "print(\"Vocab size of {} is: {} and Corpus size is: {}\".format('French', len(test_vocabulary_fr), len(test_corpus_fr)))\n",
    "print(\"Vocab size of {} is: {} and Corpus size is: {}\".format('Japanese', len(test_vocabulary_jp), len(test_corpus_jp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save to file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_train = {'en':train_en, 'jp':train_jp, 'fr':train_fr}\n",
    "save_corpus = {'en':train_corpus_en, 'fr':train_corpus_fr, 'jp':train_corpus_jp}\n",
    "save_vocab = {'en':train_vocabulary_en, 'fr':train_vocabulary_fr, 'jp':train_vocabulary_jp}\n",
    "\n",
    "\n",
    "save_train_y = {'en':y_train_en, 'jp':y_train_jp, 'fr':y_train_fr}\n",
    "save_test_y = {'en':y_test_en, 'jp':y_test_jp, 'fr':y_test_fr}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = ['en', 'jp','fr']\n",
    "if data_save == True:\n",
    "    for i in languages:\n",
    "        with open(f'../data/amazon_reviews/train/processed_data/train_tokens_{i}.pickle', 'wb') as file:\n",
    "            pickle.dump(save_train[i], file)\n",
    "        with open(f'../data/amazon_reviews/train/processed_data/train_corpus_{i}.pickle', 'wb') as file:\n",
    "            pickle.dump(save_corpus[i], file)\n",
    "        with open(f'../data/amazon_reviews/train/processed_data/train_vocab_{i}.pickle', 'wb') as file:\n",
    "           pickle.dump(save_vocab[i], file)\n",
    "        with open(f'../data/amazon_reviews/train/processed_data/y_train_{i}.pickle', 'wb') as file:\n",
    "            pickle.dump(save_train_y[i], file)\n",
    "        \n",
    "else:\n",
    "    print('Did nothing blud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_test = {'en':test_en, 'jp':test_jp, 'fr':test_fr}\n",
    "save_corpus_test = {'en':test_corpus_en, 'fr':test_corpus_fr, 'jp':test_corpus_jp}\n",
    "save_vocab_test = {'en':test_vocabulary_en, 'fr':test_vocabulary_fr, 'jp':test_vocabulary_jp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = ['en', 'jp','fr']\n",
    "if data_save == True:\n",
    "    for i in languages:\n",
    "        with open(f'../data/amazon_reviews/test/processed_data/test_tokens_{i}.pickle', 'wb') as file:\n",
    "            pickle.dump(save_test[i], file)\n",
    "        with open(f'../data/amazon_reviews/test/processed_data/test_corpus_{i}.pickle', 'wb') as file:\n",
    "            pickle.dump(save_corpus_test[i], file)\n",
    "        with open(f'../data/amazon_reviews/test/processed_data/test_vocab_{i}.pickle', 'wb') as file:\n",
    "            pickle.dump(save_vocab_test[i], file)\n",
    "        with open(f'../data/amazon_reviews/test/processed_data/y_test_{i}.pickle', 'wb') as file:\n",
    "            pickle.dump(save_test_y[i], file)\n",
    "else:\n",
    "    print('Did nothing blud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
