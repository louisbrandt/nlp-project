{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pickle \n",
    "import fugashi # pip install fugashi or pip install fugashi[unidic-lite]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data from previously saved file\n",
    "with open(f'../data/amazon_reviews/train/data_train.json', 'rb') as train_file:\n",
    "    train_prepped_data = pickle.load(train_file)\n",
    "with open(f'../data/amazon_reviews/test/data_test.json', 'rb') as file:\n",
    "    test_prepped_data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_encode(sent):\n",
    "    '''\n",
    "    Helper function to encode sentiment\n",
    "    ------\n",
    "    Takes in string description\n",
    "        'sent' - either positive or negative\n",
    "    Returns binary encoding\n",
    "        1 = positive sentiment\n",
    "        0 = negative sentiment\n",
    "    '''\n",
    "    if float(sent) > 2.5:\n",
    "        return 1 # positive\n",
    "    else:\n",
    "        return 0 # negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(data, language):\n",
    "    '''\n",
    "    Function to clean the data\n",
    "    -----\n",
    "    Takes in data set from load_data() and language needed to tokenize in\n",
    "        'data' - nested dictionary  \n",
    "    Returns two lists\n",
    "        cleaned - X list of tuples (id,[text])\n",
    "        ys - y list\n",
    "    '''\n",
    "    cleaned = [] \n",
    "    ys = []\n",
    "    tagger = fugashi.Tagger()\n",
    "\n",
    "    for idx in data:\n",
    "        review = data[idx].get('text', None) # some data does not have a review text\n",
    "        summary = data[idx].get('summary', None) # some data does not have a summary \n",
    "        \n",
    "        # combine summary and review\n",
    "        if review == None and summary == None:\n",
    "            text = ''\n",
    "        elif review == None:\n",
    "            text = summary\n",
    "        elif summary == None:\n",
    "            text = review\n",
    "        else:\n",
    "            text = summary + ' ' + review\n",
    "        \n",
    "        # tokenizing for english and french reviews\n",
    "        if language == \"english\" or language == \"french\":\n",
    "            text = text.lower()\n",
    "            sequence = word_tokenize(text, language = language)  # splits gotta into got ta\n",
    "            cleaned.append(sequence)\n",
    "            # encode sentiment\n",
    "            ys.append(sent_encode(data[idx]['rating']))\n",
    "\n",
    "        # tokenizing for japanese reviews !! NOT SET UP YET !!\n",
    "        elif language == \"japanese\":\n",
    "            text = text.lower()\n",
    "            sequence = [word.surface for word in tagger(text)]\n",
    "            cleaned.append(sequence)\n",
    "            # encode sentiment\n",
    "            ys.append(sent_encode(data[idx]['rating']))\n",
    "\n",
    "    return cleaned, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_corpus(dataset):\n",
    "    '''\n",
    "    Function computing vocabluary and corpus for a dataset\n",
    "    -----\n",
    "    Takes a cleaned dataset - list \n",
    "        dataset - X list \n",
    "    Returns\n",
    "        vocab - set of unique tokens in dataset\n",
    "        corpus - list of strings; sentences in dataset \n",
    "    '''\n",
    "    vocab = set()\n",
    "    corpus = []\n",
    "    for text in dataset: # for list in list of lists\n",
    "        sentence = ''\n",
    "        for token in text: # for token in list \n",
    "            vocab.add(token)\n",
    "            if token in ['.','!','?',',',';',':']:\n",
    "                sentence += token \n",
    "            else:\n",
    "                sentence += ' ' + token \n",
    "        corpus.append(sentence.lstrip()) \n",
    "    return vocab, corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering for the first 50.000 instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = ['en', 'jp','fr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the data to only include 50,000 reviews from each language\n",
    "eng50000 = dict(list(train_prepped_data['en'].items())[0:50000])\n",
    "frn50000 = dict(list(train_prepped_data['fr'].items())[0:50000])\n",
    "jap50000 = dict(list(train_prepped_data['jp'].items())[0:50000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_en, y_train_en = clean(eng50000, \"english\") # english tokenized text\n",
    "train_fr, y_train_fr = clean(frn50000, \"french\") # french tokenized text\n",
    "train_jp, y_train_jp = clean(jap50000, \"japanese\") # japanese tokenized text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview of positive/negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = 0\n",
    "negative = 0\n",
    "for i in y_train_en:\n",
    "    if i == 1:\n",
    "        positive += 1\n",
    "    elif i == 0:\n",
    "        negative += 1 \n",
    "\n",
    "print(\"Number of positive reviews (English): \", positive)\n",
    "print(\"Number of negative reviews (English): \", negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = 0\n",
    "negative = 0\n",
    "for i in y_train_fr:\n",
    "    if i == 1:\n",
    "        positive += 1\n",
    "    elif i == 0:\n",
    "        negative += 1 \n",
    "\n",
    "print(\"Number of positive reviews (French): \", positive)\n",
    "print(\"Number of negative reviews (French): \", negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = 0\n",
    "negative = 0\n",
    "for i in y_train_jp:\n",
    "    if i == 1:\n",
    "        positive += 1\n",
    "    elif i == 0:\n",
    "        negative += 1 \n",
    "\n",
    "print(\"Number of positive reviews (Japanese): \", positive)\n",
    "print(\"Number of negative reviews (Japanese): \", negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocab & Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocabulary_en, train_corpus_en = get_vocab_corpus(train_en)\n",
    "train_vocabulary_fr, train_corpus_fr = get_vocab_corpus(train_fr)\n",
    "train_vocabulary_jp, train_corpus_jp = get_vocab_corpus(train_jp)\n",
    "\n",
    "print(\"Vocab size of {} is: {} and Corpus size is: {}\".format('English', len(train_vocabulary_en), len(train_corpus_en)))\n",
    "print(\"Vocab size of {} is: {} and Corpus size is: {}\".format('French', len(train_vocabulary_fr), len(train_corpus_fr)))\n",
    "print(\"Vocab size of {} is: {} and Corpus size is: {}\".format('Japanese', len(train_vocabulary_jp), len(train_corpus_jp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
