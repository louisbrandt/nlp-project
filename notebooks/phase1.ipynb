{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Baseline \n",
    "The goal of this phase is to create a baseline model. Note that the word baseline can mean different things. In the course we distinguished three different types of baselines:\n",
    "\n",
    "1. The simplest possible approach (majority baseline, i.e. everything is positive or noun)\n",
    "2. A simple machine learning classifier (logistic regression with words as features)\n",
    "3. The 'state-of-the-art' approach on which you want to improve (your starting point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Sentiment classification\n",
    "The data can be found in the **classification folder** .  \n",
    "The goal is to **predict the label** in the sentiment field.  \n",
    "You have to upload the predictions of music_reviews_test_masked.json.gz to CodaLab. (The link will be posted here on monday). Note that the format should match the json files in the repository.  \n",
    "Also upload a .txt file on LearnIt (one per group) with a short description of your baseline.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0 - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "7164a602-f830-4cd2-a56b-dc41909129de",
    "deepnote_cell_height": 297,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     null,
     21.1875
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1731,
    "execution_start": 1647941551254,
    "source_hash": "d07def82",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "run = False\n",
    "if run == True: \n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Data Preprocessing\n",
    "\n",
    "### 1.1 - Load the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = {'train':'../data/classification/music_reviews_train.json.gz',\n",
    "        'dev': '../data/classification/music_reviews_dev.json.gz',\n",
    "        'test': '../data/classification/music_reviews_test_masked.json.gz'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    '''\n",
    "    Function to load the data\n",
    "    -----\n",
    "    Takes in the argument: \n",
    "        'path' - takes the form PATH['(train, dev or test)']\n",
    "    '''\n",
    "    dic = {}\n",
    "    for i, line in enumerate(gzip.open(path)):\n",
    "        review_data = json.loads(line)\n",
    "        dic[i] = {}\n",
    "        for key,value in review_data.items():\n",
    "            dic[i][key] = value\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e90c76-a10b-4fee-ac9a-c2c1e7a8deb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_data(PATH['train'])\n",
    "dev_data = load_data(PATH['dev'])\n",
    "test_data = load_data(PATH['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_encode(sent):\n",
    "    if sent == 'positive':\n",
    "        return 1\n",
    "    if sent == 'negative':\n",
    "        return 0 \n",
    "    return 'unknown sentiment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(data):\n",
    "    stop = stopwords.words('english')\n",
    "    cleaned = {}\n",
    "    ys = []\n",
    "    for idx in data:\n",
    "        review = data[idx].get('reviewText', None) # some data does not have a review text\n",
    "        summary = data[idx].get('summary', None) # some data does not have a summary \n",
    "        \n",
    "        # combine summary and review\n",
    "        if review == None and summary == None:\n",
    "            continue\n",
    "        elif review == None:\n",
    "            text = summary\n",
    "        elif summary == None:\n",
    "            text = review\n",
    "        else:\n",
    "            text = summary + ' ' + review\n",
    "\n",
    "        # remove stop words\n",
    "        seq = []\n",
    "        sequence = word_tokenize(text)  # splits gotta into got ta\n",
    "        for token in sequence:\n",
    "            token = token.lower()\n",
    "            if token not in stop:\n",
    "                seq.append(token)\n",
    "        cleaned[idx] = {}\n",
    "        cleaned[idx]['text'] = seq\n",
    "\n",
    "        # encode sentiment\n",
    "        ys.append(sent_encode(data[idx]['sentiment']))\n",
    "\n",
    "        # ys\n",
    "    return cleaned, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_train, y_train = clean(train_data)\n",
    "cleaned_dev, y_dev = clean(dev_data)\n",
    "cleaned_test, _ = clean(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Vocab & Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_corpus(train):\n",
    "    vocab = set()\n",
    "    corpus = []\n",
    "    for idx in train:\n",
    "        text = train[idx]['text']\n",
    "        sentence = ''\n",
    "        for token in text:\n",
    "            vocab.add(token)\n",
    "            if token in ['.','!','?',',']:\n",
    "                sentence += token \n",
    "            else:\n",
    "                sentence += ' ' + token \n",
    "        corpus.append(sentence.lstrip()) \n",
    "    return vocab, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocabulary, train_corpus = get_vocab_corpus(cleaned_train)\n",
    "dev_vocabulary, dev_corpus = get_vocab_corpus(cleaned_dev)\n",
    "test_vocabulary, test_corpus = get_vocab_corpus(cleaned_test) # dev and test vocab not used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bow(vocab, corp):\n",
    "    vocab = {word:i for i,word in enumerate(vocab)}\n",
    "    vectorizer = TfidfVectorizer(vocabulary= vocab)\n",
    "    bow = vectorizer.fit_transform(corp) \n",
    "    return bow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bow = get_bow(train_vocabulary,train_corpus)\n",
    "dev_bow = get_bow(train_vocabulary,dev_corpus)\n",
    "test_bow = get_bow(train_vocabulary,test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Train the Model\n",
    "\n",
    "### 2.1 Fit Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(train_bow.toarray(), np.array(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(dev_bow,y_dev)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
