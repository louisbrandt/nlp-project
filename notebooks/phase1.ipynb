{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Baseline \n",
    "The goal of this phase is to create a baseline model. Note that the word baseline can mean different things. In the course we distinguished three different types of baselines:\n",
    "\n",
    "1. The simplest possible approach (majority baseline, i.e. everything is positive or noun)\n",
    "2. A simple machine learning classifier (logistic regression with words as features)\n",
    "3. The 'state-of-the-art' approach on which you want to improve (your starting point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Sentiment classification\n",
    "The data can be found in the **classification folder** .  \n",
    "The goal is to **predict the label** in the sentiment field.  \n",
    "You have to upload the predictions of music_reviews_test_masked.json.gz to CodaLab. (The link will be posted here on monday). Note that the format should match the json files in the repository.  \n",
    "Also upload a .txt file on LearnIt (one per group) with a short description of your baseline.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0 - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "7164a602-f830-4cd2-a56b-dc41909129de",
    "deepnote_cell_height": 297,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     null,
     21.1875
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1731,
    "execution_start": 1647941551254,
    "source_hash": "d07def82",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Data Preprocessing\n",
    "\n",
    "1.1 - Load the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = {'train':'../data/classification/music_reviews_train.json.gz',\n",
    "        'dev': '../data/classification/music_reviews_dev.json.gz',\n",
    "        'test': '../data/classification/music_reviews_test_masked.json.gz'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    '''\n",
    "    Function to load the data\n",
    "    -----\n",
    "    Takes in the argument: \n",
    "        'path' - takes the form PATH['(train, dev or test)']\n",
    "    '''\n",
    "    dic = {}\n",
    "    for i, line in enumerate(gzip.open(path)):\n",
    "        review_data = json.loads(line)\n",
    "        dic[i] = {}\n",
    "        for key,value in review_data.items():\n",
    "            dic[i][key] = value\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e90c76-a10b-4fee-ac9a-c2c1e7a8deb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_data(PATH['train'])\n",
    "dev_data = load_data(PATH['dev'])\n",
    "test_data = load_data(PATH['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 - Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_encode(sent):\n",
    "    '''\n",
    "    Helper function to encode sentiment\n",
    "    ------\n",
    "    Takes in string description\n",
    "        'sent' - either positive or negative\n",
    "    Returns binary encoding\n",
    "        1 = positive sentiment\n",
    "        0 = negative sentiment\n",
    "    '''\n",
    "    if sent == 'positive':\n",
    "        return 1\n",
    "    if sent == 'negative':\n",
    "        return 0 \n",
    "    return 'unknown sentiment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(data):\n",
    "    '''\n",
    "    Function to clean the data\n",
    "    -----\n",
    "    Takes in data set from load_data()\n",
    "        'data' - nested dictionary  \n",
    "    Returns two lists\n",
    "        cleaned - X list\n",
    "        ys - y list\n",
    "    '''\n",
    "    cleaned = [] \n",
    "    ys = []\n",
    "    for idx in data:\n",
    "        review = data[idx].get('reviewText', None) # some data does not have a review text\n",
    "        summary = data[idx].get('summary', None) # some data does not have a summary \n",
    "        \n",
    "        # combine summary and review\n",
    "        if review == None and summary == None:\n",
    "            continue\n",
    "        elif review == None:\n",
    "            text = summary\n",
    "        elif summary == None:\n",
    "            text = review\n",
    "        else:\n",
    "            text = summary + ' ' + review\n",
    "\n",
    "        sequence = word_tokenize(text)  # splits gotta into got ta\n",
    "        cleaned.append(sequence)\n",
    "\n",
    "        # encode sentiment\n",
    "        ys.append(sent_encode(data[idx]['sentiment']))\n",
    "\n",
    "    return cleaned, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_train, y_train = clean(train_data)\n",
    "cleaned_dev, y_dev = clean(dev_data)\n",
    "cleaned_test, _ = clean(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Vocab & Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_corpus(dataset):\n",
    "    '''\n",
    "    Function computing vocabluary and corpus for a dataset\n",
    "    -----\n",
    "    Takes a cleaned dataset - list \n",
    "        dataset - X list \n",
    "    Returns\n",
    "        vocab - set of unique tokens in dataset\n",
    "        corpus - list of strings; sentences in dataset \n",
    "    '''\n",
    "    vocab = set()\n",
    "    corpus = []\n",
    "    for text in dataset:\n",
    "        sentence = ''\n",
    "        for token in text:\n",
    "            vocab.add(token)\n",
    "            if token in ['.','!','?',',']:\n",
    "                sentence += token \n",
    "            else:\n",
    "                sentence += ' ' + token \n",
    "        corpus.append(sentence.lstrip()) \n",
    "    return vocab, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocabulary, train_corpus = get_vocab_corpus(cleaned_train)\n",
    "dev_vocabulary, dev_corpus = get_vocab_corpus(cleaned_dev)\n",
    "test_vocabulary, test_corpus = get_vocab_corpus(cleaned_test) # dev and test vocab not used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4 Combine train and dev for cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindev_corpus = train_corpus + dev_corpus\n",
    "y_traindev = y_train + y_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5 Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bow(vocab, corp):\n",
    "    '''\n",
    "    Function returning sparse matrix of Term Frequency â€” Inverse Document Frequencies\n",
    "    -----\n",
    "    Takes vocab and corpus, working with two lists\n",
    "        vocab - set of unique words\n",
    "        corpus - list of strings\n",
    "    Returns bag of words\n",
    "        bow - 2d matrix; input to model\n",
    "    '''\n",
    "    vocab = list(vocab) \n",
    "    vectorizer = TfidfVectorizer(vocabulary= vocab)\n",
    "    bow = vectorizer.fit_transform(corp) \n",
    "    return bow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bow = get_bow(train_vocabulary,traindev_corpus)\n",
    "test_bow = get_bow(train_vocabulary,test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Run the Model\n",
    "\n",
    "2.1 Grid search of train and dev data to find best fitting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "parameters = {'max_iter':[100,500,1000], 'C': [1,2,3,4]}\n",
    "grid = GridSearchCV(lr, parameters)\n",
    "grid.fit(train_bow, y_traindev)\n",
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Predict on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(test_bow)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4 Report test scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_encode(sent):\n",
    "    if sent == 1:\n",
    "        return 'positive'\n",
    "    if sent == 0:\n",
    "        return 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_test(test, ys):\n",
    "    '''\n",
    "    Function to insert predicitons into test data\n",
    "    '''\n",
    "    index = 0\n",
    "    for key in test:\n",
    "        review = test[key].get('reviewText', None) \n",
    "        summary = test[key].get('summary', None) \n",
    "        if review == None and summary == None:\n",
    "            continue\n",
    "        test[key]['sentiment'] = reverse_encode(ys[index])\n",
    "        index += 1\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finished_test_data = pred_test(test_data,y_pred)\n",
    "finished_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_json=[json.dumps(i)+'\\n' for i in finished_test_data.values()]\n",
    "#with open ('music_reviews_test.json', 'w') as file:\n",
    "#    file.writelines(test_json)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
