{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Year Project\n",
    "## Natural Language Processing\n",
    "Group 10 -  Fillip Due, Andreas Olsen, Louis Brandt, Emma Bisgaard\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "Describe siUU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import json\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = {'train':'../data/music_reviews_train.json.gz',\n",
    "        'dev': '../data/music_reviews_dev.json.gz',\n",
    "        'test': '../data/music_reviews_test_masked.json.gz'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    '''\n",
    "    Function to load the data from json.gz\n",
    "    -----\n",
    "    Takes in the argument: \n",
    "        'path' - takes the form PATH['(train, dev or test)']\n",
    "    '''\n",
    "    dic = {}\n",
    "    for i, line in enumerate(gzip.open(path)):\n",
    "        review_data = json.loads(line)\n",
    "        dic[i] = {}\n",
    "        for key,value in review_data.items():\n",
    "            dic[i][key] = value\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_data(PATH['train'])\n",
    "dev_data = load_data(PATH['dev'])\n",
    "test_data = load_data(PATH['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_encode(sent):\n",
    "    '''\n",
    "    Helper function to encode sentiment\n",
    "    ------\n",
    "    Takes in string description\n",
    "        'sent' - either positive or negative\n",
    "    Returns binary encoding\n",
    "        1 = positive sentiment\n",
    "        0 = negative sentiment\n",
    "    '''\n",
    "    if sent == 'positive':\n",
    "        return 1\n",
    "    if sent == 'negative':\n",
    "        return 0 \n",
    "    return '_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(data):\n",
    "    '''\n",
    "    Function to clean the data\n",
    "    -----\n",
    "    Takes in data set from load_data()\n",
    "        'data' - nested dictionary  \n",
    "    Returns two lists\n",
    "        cleaned - X list of tuples (id,[text])\n",
    "        ys - y list\n",
    "    '''\n",
    "    cleaned = [] \n",
    "    ys = []\n",
    "    for idx in data:\n",
    "        review = data[idx].get('reviewText', None) # some data does not have a review text\n",
    "        summary = data[idx].get('summary', None) # some data does not have a summary \n",
    "        \n",
    "        # combine summary and review\n",
    "        if review == None and summary == None:\n",
    "            text = ''\n",
    "        elif review == None:\n",
    "            text = summary\n",
    "        elif summary == None:\n",
    "            text = review\n",
    "        else:\n",
    "            text = summary + ' ' + review\n",
    "        text = text.lower()\n",
    "        sequence = word_tokenize(text)  # splits gotta into got ta\n",
    "        cleaned.append(sequence)\n",
    "\n",
    "        # encode sentiment\n",
    "        ys.append(sent_encode(data[idx]['sentiment']))\n",
    "\n",
    "    return cleaned, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean, y_train = clean(train_data)\n",
    "dev_clean, y_dev = clean(dev_data)\n",
    "test_clean, _ = clean(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine dev and train for cross validation\n",
    "train_clean += dev_clean    # id for test set < 100000 \n",
    "y_train += y_dev            # id for dev set > 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most common tokens throughout data \n",
    "from collections import Counter\n",
    "a = train_clean + test_clean\n",
    "l = []\n",
    "for x in a:\n",
    "    for _ in x:\n",
    "        l.append(_)\n",
    "c = Counter(l)\n",
    "c.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Generating Difficult Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3.1 - Positive Negative Synoyms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_adj = ['good', 'great', 'excellent', 'amazing', 'extraordinary', 'beautiful', 'fantastic', 'nice', 'incredible', 'exceptional', 'awesome', 'perfect', 'fun', 'happy', 'adorable', 'brilliant', 'exciting', 'sweet', 'wonderful','stupendous','helpful']\n",
    "neg_adj = ['awful', 'bad', 'horrible', 'weird', 'rough', 'lousy', 'unhappy', 'average', 'difficult', 'poor', 'sad', 'frustrating', 'hard', 'lame', 'nasty', 'annoying', 'boring', 'creepy', 'dreadful', 'ridiculous', 'terrible', 'ugly', 'unpleasant','shame']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_synonyms(data):\n",
    "    '''\n",
    "    Function which generates data where positive/negative adjectives are replaced with synonyms\n",
    "    -------\n",
    "    Takes in cleaned data\n",
    "    Returns new instances\n",
    "    '''\n",
    "    new = []\n",
    "    indexes= []\n",
    "    ys = []\n",
    "    t = 0\n",
    "    i = 0\n",
    "    while t < 300: # number of cases generated\n",
    "        c = False\n",
    "        text = data[i].copy()\n",
    "        for idx, token in enumerate(text):\n",
    "            if token in pos_adj:\n",
    "                text[idx] = random.choice(pos_adj)\n",
    "                c = True\n",
    "            if token in neg_adj:\n",
    "                text[idx] = random.choice(neg_adj)\n",
    "                c = True\n",
    "        if c: \n",
    "            new.append(text)\n",
    "            indexes.append(i)\n",
    "            t +=1\n",
    "            ys.append(y_train[i])\n",
    "        i += 1\n",
    "        \n",
    "    return new, indexes, ys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms, indexes_synonyms, y_synonyms  = gen_synonyms(train_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3.2 - Not Variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_swap(sent):\n",
    "    if sent == 1:\n",
    "        return 0\n",
    "    if sent == 0:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_nots(data):\n",
    "    '''\n",
    "    Function which generates data where nots are removed\n",
    "    -------\n",
    "    Takes in cleaned data\n",
    "    Returns new instances\n",
    "    '''\n",
    "    new = []\n",
    "    indexes= []\n",
    "    ys = []\n",
    "    t = 0\n",
    "    i = 0\n",
    "    while t < 300: # number of cases generated\n",
    "        c = False\n",
    "        text = data[i].copy()\n",
    "        for idx, token in enumerate(text):\n",
    "            if token in ['not','n\\'t']:\n",
    "                text[idx] = ''\n",
    "                c = True\n",
    "        if c: \n",
    "            new.append(text)\n",
    "            indexes.append(i)\n",
    "            t +=1\n",
    "            ys.append(sent_swap(y_train[i]))\n",
    "        i += 1\n",
    "    return new, indexes, ys \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nots, indexes_nots, y_nots = gen_nots(train_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3.3 - Name Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_NER(data:list, no_instances):\n",
    "    '''\n",
    "    This function takes in dataset in json format as we know it, and replaces the text where the sequence lables are PERSONS to \n",
    "    whatever name given in the list. This is done in correspondance with the Named Entity Extraction by sequence labeling each word\n",
    "    to then extract and exchange the ones that are of the tag Proper Noun and PERSON.\n",
    "    This is done with the help from the library en_core_web from Spacy which needs to be installed.\n",
    "    https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da\n",
    "    '''\n",
    "    working_data = data.copy()    \n",
    "    \n",
    "    nlp = en_core_web_sm.load()\n",
    "    \n",
    "    ## find reviews with persons in them, to actually impact the reviews.\n",
    "    reviews_to_change = {}\n",
    "    for i in range(2000):\n",
    "        sequence = ' '.join(working_data[i])\n",
    "        doc = nlp(sequence) # generate sequence labeling and entity extraction\n",
    "        for token in doc.ents:\n",
    "            if token.label_ == 'PERSON':\n",
    "                reviews_to_change[i] = sequence\n",
    "            else: \n",
    "                continue\n",
    "        if len(reviews_to_change) == no_instances:\n",
    "            break\n",
    "            \n",
    "    ## change persons to ridiculous/random names to confuse the model consisting purposefully of adjectives, nouns and proper nouns\n",
    "    names = ['Louis Top Boy', 'Andreas Bad Energy', 'Good will Bisgaard', 'Filip Fine', 'rolling peppers', 'The Animals', 'Black Sabbath', 'Jens Jensen', 'excellent Grasshoppers', 'butter soft harlekins', 'Basil', 'filet o\\' fish']\n",
    "    for i in reviews_to_change:\n",
    "        doc = nlp(reviews_to_change[i])\n",
    "        for token in doc.ents:\n",
    "            if token.label_ == 'PERSON':\n",
    "                replace = random.choice(names)\n",
    "                reviews_to_change[i] = reviews_to_change[i].replace(token.text, replace)\n",
    "    indexes = []\n",
    "    new = []\n",
    "    ys = []\n",
    "    for i, string in reviews_to_change.items():\n",
    "        indexes.append(i)\n",
    "        new.append(string.split())\n",
    "        ys.append(y_train[i])\n",
    "    return new, indexes, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ners, indexes_ners, y_ners = gen_NER(train_clean,300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 - Generate Vocab, Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_corpus(dataset):\n",
    "    '''\n",
    "    Function computing vocabluary and corpus for a dataset\n",
    "    -----\n",
    "    Takes a cleaned dataset - list \n",
    "        dataset - X list \n",
    "    Returns\n",
    "        vocab - set of unique tokens in dataset\n",
    "        corpus - list of strings; sentences in dataset \n",
    "    '''\n",
    "    vocab = set()\n",
    "    corpus = []\n",
    "    for text in dataset: # for list in list of lists\n",
    "        sentence = ''\n",
    "        for token in text: # for token in list \n",
    "            vocab.add(token)\n",
    "            if token in ['.','!','?',',',';',':']:\n",
    "                sentence += token \n",
    "            else:\n",
    "                sentence += ' ' + token \n",
    "        corpus.append(sentence.lstrip()) \n",
    "    return vocab, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocabulary, train_corpus = get_vocab_corpus(train_clean)\n",
    "test_vocabulary, test_corpus = get_vocab_corpus(test_clean) # test vocab not used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 - Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bow(vocab, corp):\n",
    "    '''\n",
    "    Function returning sparse matrix of Term Frequency â€” Inverse Document Frequencies\n",
    "    -----\n",
    "    Takes vocab and corpus, working with two lists\n",
    "        vocab - set of unique words\n",
    "        corpus - list of strings\n",
    "    Returns bag of words\n",
    "        bow - 2d matrix; input to model\n",
    "    '''\n",
    "    vocab = list(vocab) \n",
    "    vectorizer = TfidfVectorizer(vocabulary= vocab)\n",
    "    bow = vectorizer.fit_transform(corp) \n",
    "    return bow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bow = get_bow(train_vocabulary,train_corpus)\n",
    "test_bow = get_bow(train_vocabulary,test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Fit training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "parameters = {'max_iter':[100,500], 'C': [2,3,4,5]}\n",
    "grid = GridSearchCV(lr, parameters)\n",
    "grid.fit(train_bow, y_train)\n",
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = grid.best_estimator_\n",
    "scores = cross_val_score(estimator= baseline_model,X= train_bow,y= y_train)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 - Test Prediciton and Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_predictions = baseline_model.predict(test_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_test(test, ys):\n",
    "    '''\n",
    "    Function to insert predicitons into test data\n",
    "    '''\n",
    "    index = 0\n",
    "    for key in test:\n",
    "        test[key]['sentiment'] = reverse_encode(ys[index])\n",
    "        index += 1\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_encode(sent):\n",
    "    if sent == 1:\n",
    "        return 'positive'\n",
    "    if sent == 0:\n",
    "        return 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finished_test_data = pred_test(test_data,baseline_predictions)\n",
    "finished_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 - Write predicitons to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json(dict,filename): \n",
    "    dict_json=[json.dumps(i)+'\\n' for i in dict.values()]\n",
    "    with open (filename, 'a') as file:\n",
    "        file.writelines(dict_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_text(data, new, indexes, cat, ys, is_nots=False):\n",
    "    d = copy.deepcopy(data) \n",
    "    d = {key:val for key,val in d.items() if key in indexes}\n",
    "    for iidx, idx in enumerate(indexes):\n",
    "        d[idx]['reviewText'] = ' '.join([str(tok) for tok in new[iidx]])\n",
    "        d[idx]['category'] = cat\n",
    "        if is_nots: d[idx]['sentiment'] = reverse_encode(ys[iidx])\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_syn = insert_text(train_data,synonyms,indexes_synonyms,'synonym', y_synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nots = insert_text(train_data, nots, indexes_nots, 'negation', y_nots, is_nots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ners = insert_text(train_data, ners, indexes_ners, 'ner', y_ners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_json(train_nots,'group10.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_json(train_syn,'group10.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_json(train_ners,'group10.json')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
