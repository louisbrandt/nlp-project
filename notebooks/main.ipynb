{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Year Project\n",
    "## Natural Language Processing\n",
    "Group 10 -  Fillip Due, Andreas Olsen, Louis Brandt, Emma Bisgaard\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "Describe siUU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = {'train':'../data/music_reviews_train.json.gz',\n",
    "        'dev': '../data/music_reviews_dev.json.gz',\n",
    "        'test': '../data/music_reviews_test_masked.json.gz'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    '''\n",
    "    Function to load the data from json.gz\n",
    "    -----\n",
    "    Takes in the argument: \n",
    "        'path' - takes the form PATH['(train, dev or test)']\n",
    "    '''\n",
    "    dic = {}\n",
    "    for i, line in enumerate(gzip.open(path)):\n",
    "        review_data = json.loads(line)\n",
    "        dic[i] = {}\n",
    "        for key,value in review_data.items():\n",
    "            dic[i][key] = value\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_data(PATH['train'])\n",
    "dev_data = load_data(PATH['dev'])\n",
    "test_data = load_data(PATH['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_encode(sent):\n",
    "    '''\n",
    "    Helper function to encode sentiment\n",
    "    ------\n",
    "    Takes in string description\n",
    "        'sent' - either positive or negative\n",
    "    Returns binary encoding\n",
    "        1 = positive sentiment\n",
    "        0 = negative sentiment\n",
    "    '''\n",
    "    if sent == 'positive':\n",
    "        return 1\n",
    "    if sent == 'negative':\n",
    "        return 0 \n",
    "    return '_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(data):\n",
    "    '''\n",
    "    Function to clean the data\n",
    "    -----\n",
    "    Takes in data set from load_data()\n",
    "        'data' - nested dictionary  \n",
    "    Returns two lists\n",
    "        cleaned - X list of tuples (id,[text])\n",
    "        ys - y list\n",
    "    '''\n",
    "    cleaned = [] \n",
    "    ys = []\n",
    "    for idx in data:\n",
    "        review = data[idx].get('reviewText', None) # some data does not have a review text\n",
    "        summary = data[idx].get('summary', None) # some data does not have a summary \n",
    "        \n",
    "        # combine summary and review\n",
    "        if review == None and summary == None:\n",
    "            text = ''\n",
    "        elif review == None:\n",
    "            text = summary\n",
    "        elif summary == None:\n",
    "            text = review\n",
    "        else:\n",
    "            text = summary + ' ' + review\n",
    "\n",
    "        sequence = word_tokenize(text)  # splits gotta into got ta\n",
    "        cleaned.append(sequence)\n",
    "\n",
    "        # encode sentiment\n",
    "        ys.append(sent_encode(data[idx]['sentiment']))\n",
    "\n",
    "    return cleaned, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean, y_train = clean(train_data)\n",
    "dev_clean, y_dev = clean(dev_data)\n",
    "test_clean, _ = clean(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine dev and train for cross validation\n",
    "train_clean += dev_clean    # id for test set < 100000 \n",
    "y_train += y_dev            # id for dev set > 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most common tokens throughout data \n",
    "from collections import Counter\n",
    "a = train_clean + test_clean\n",
    "l = []\n",
    "for x in a:\n",
    "    for _ in x:\n",
    "        l.append(_)\n",
    "c = Counter(l)\n",
    "c.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Generate Vocab, Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_corpus(dataset):\n",
    "    '''\n",
    "    Function computing vocabluary and corpus for a dataset\n",
    "    -----\n",
    "    Takes a cleaned dataset - list \n",
    "        dataset - X list \n",
    "    Returns\n",
    "        vocab - set of unique tokens in dataset\n",
    "        corpus - list of strings; sentences in dataset \n",
    "    '''\n",
    "    vocab = set()\n",
    "    corpus = []\n",
    "    for text in dataset: # for list in list of lists\n",
    "        sentence = ''\n",
    "        for token in text: # for token in list \n",
    "            vocab.add(token)\n",
    "            if token in ['.','!','?',',',';',':']:\n",
    "                sentence += token \n",
    "            else:\n",
    "                sentence += ' ' + token \n",
    "        corpus.append(sentence.lstrip()) \n",
    "    return vocab, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocabulary, train_corpus = get_vocab_corpus(train_clean)\n",
    "test_vocabulary, test_corpus = get_vocab_corpus(test_clean) # test vocab not used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4 Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bow(vocab, corp):\n",
    "    '''\n",
    "    Function returning sparse matrix of Term Frequency â€” Inverse Document Frequencies\n",
    "    -----\n",
    "    Takes vocab and corpus, working with two lists\n",
    "        vocab - set of unique words\n",
    "        corpus - list of strings\n",
    "    Returns bag of words\n",
    "        bow - 2d matrix; input to model\n",
    "    '''\n",
    "    vocab = list(vocab) \n",
    "    vectorizer = TfidfVectorizer(vocabulary= vocab)\n",
    "    bow = vectorizer.fit_transform(corp) \n",
    "    return bow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bow = get_bow(train_vocabulary,train_corpus)\n",
    "test_bow = get_bow(train_vocabulary,test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Generating Difficult Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Fit training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "parameters = {'max_iter':[100,500], 'C': [2,3,4,5]}\n",
    "grid = GridSearchCV(lr, parameters)\n",
    "grid.fit(train_bow, y_train)\n",
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = grid.best_estimator_\n",
    "scores = cross_val_score(estimator= baseline_model,X= train_bow,y= y_train)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4 Test Prediciton and Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_predictions = baseline_model.predict(test_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_test(test, ys):\n",
    "    '''\n",
    "    Function to insert predicitons into test data\n",
    "    '''\n",
    "    index = 0\n",
    "    for key in test:\n",
    "        test[key]['sentiment'] = reverse_encode(ys[index])\n",
    "        index += 1\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_encode(sent):\n",
    "    if sent == 1:\n",
    "        return 'positive'\n",
    "    if sent == 0:\n",
    "        return 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finished_test_data = pred_test(test_data,baseline_predictions)\n",
    "finished_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5 Write predicitons to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_json=[json.dumps(i)+'\\n' for i in finished_test_data.values()]\n",
    "# with open ('music_reviews_test.json', 'w') as file:\n",
    "# file.writelines(test_json)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
