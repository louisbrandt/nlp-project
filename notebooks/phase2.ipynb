{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Generation of Difficult Cases\n",
    "\n",
    "The goal of this phase is to generate difficult instances for the task of sentiment analysis. The requirements are slightly different for both task types (classification versus sequence labeling), pick the task that you build your baseline model for in phase 1.\n",
    "\n",
    "You should in both situations participate in assignment 3. In other words, you will either do assignment 1 and 3 or assignment 2 and 3.\n",
    "\n",
    "\n",
    "#### How to Generate the Samples\n",
    "There are three main methods to generate the samples:\n",
    "* You can use the Checklist paper code: https://github.com/marcotcr/checklist\n",
    "* You can write code yourself to generate the samples. You can make use of any method you prefer, including a POS-tagger, word embeddings and contextualized embeddings\n",
    "* You can generate samples manually\n",
    "\n",
    "For each of these strategies you should think of a variety of types of difficult cases (so that not the whole set contains of the same types of samples), like the categories in Table 1 in \"the Checklist paper\".\n",
    "\n",
    "Note that you have to shortly present your approach in week14 (before the project proposal, you will get 2 minutes for phase 2 and 5 for the project proposal)\n",
    "\n",
    "#### For Inspiration:\n",
    "* [Beyond Accuracy: Behavioral Testing of NLP Models with CheckList](https://www.aclweb.org/anthology/2020.acl-main.442.pdf)\n",
    "* [Towards Linguistically Generalizable NLP Systems: A Workshop and Shared Task](https://www.aclweb.org/anthology/W17-5401.pdf)\n",
    "* [Breaking NLP: Using Morphosyntax, Semantics, Pragmatics and World\n",
    "Knowledge to Fool Sentiment Analysis Systems](https://www.aclweb.org/anthology/W17-5405.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classification\n",
    "\n",
    "The formal requirements are:\n",
    "\n",
    "* 100-1000 utterances should be handed in on **LearnIt before 30-03 11:59AM**\n",
    "* Must be in the same format as the training data : one (json) dict per line, and per instance needs at least: \"reviewText\", \"sentiment\", and \"category\" key.\n",
    "* The \"category\" key indicates which type of alternation/difficulty you included.\n",
    "* The gold labels must be correct!\n",
    "\n",
    "Assuming you write a function that generates examples, writing the final file can be done like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def swap(sentiment):\n",
    "    if sentiment == 'positive':\n",
    "        return 'negative'\n",
    "    elif sentiment == 'negative':\n",
    "        return 'positive'\n",
    "\n",
    "def dataGenerator(inputSents):\n",
    "    outputSents = []\n",
    "    for instance in inputSents:\n",
    "        if 'great' in instance[0]:\n",
    "            outputSents.append({'reviewText': instance[0].replace('great', 'not great'), 'sentiment': swap(instance[1]), 'category': 'negation'})\n",
    "    return outputSents\n",
    "\n",
    "inputSents = [['this is a great album', 'positive']]\n",
    "\n",
    "#outFile = open('group13.json', 'w')\n",
    "#for instance in dataGenerator(inputSents):\n",
    "   # # goldLabel is a string, either 'positive' or 'negative', text contains the review, and category \n",
    "  #  # indicates the type of alternation you did.\n",
    " #   outFile.write(json.dumps(instance) + '\\n')\n",
    "#outFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should check whether your final file is in the correct format with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "inputPath = 'group13.json'\n",
    "\n",
    "for lineIdx, line in enumerate(open(inputPath)):\n",
    "    try:\n",
    "        data = json.loads(line)\n",
    "    except ValueError as e:\n",
    "        print('error, instance ' + str(lineIdx+1) + ' is not in valid json format')\n",
    "        continue\n",
    "    if 'reviewText' not in data:\n",
    "        print(\"error, instance \" + str(lineIdx+1) + ' does not contain key \"reviewText\"')\n",
    "        continue\n",
    "    if 'sentiment' not in data:\n",
    "        print(\"error, instance \" + str(lineIdx+1) + ' does not contain key \"sentiment\"')\n",
    "        continue\n",
    "    if data['sentiment'] not in ['positive', 'negative']:\n",
    "        print(\"error, instance \" + str(lineIdx+1) + ': sentiment is not positive/negative')\n",
    "        continue\n",
    "        \n",
    "if lineIdx+1 < 100:\n",
    "    print('Too little instances(' + str(lineIdx) + '), please generate more')\n",
    "if lineIdx+1 > 1000:\n",
    "    print('Too many instances(' + str(lineIdx) + '), please generate more')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prediction\n",
    "06-04 11:59AM is the deadline for handing in the predictions of the baseline on the difficult cases of all the groups. The datafile will be made available as soon as possible after your hand-ins (we aim for 02-04), and all you have to do is re-run your baseline from phase 1. Note that some of the meta-information might not be available, so if your baseline relies on those you have to either retrain without these features, or predict without these features.\n",
    "\n",
    "The codalab link will appear here, and will be posted on slack when available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create same structure as data \n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = {'train':'../data/music/music_reviews_train.json.gz',\n",
    "        'dev': '../data/music/music_reviews_dev.json.gz',\n",
    "        'test': '../data/music/music_reviews_test_masked.json.gz',\n",
    "        'difficult': '../data/difficult_cases/phase2_testData-masked.txt'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    '''\n",
    "    Function to load the data\n",
    "    -----\n",
    "    Takes in the argument: \n",
    "        'path' - takes the form PATH['(train, dev or test)']\n",
    "    '''\n",
    "    dic = {}\n",
    "    for i, line in enumerate(gzip.open(path)):\n",
    "        review_data = json.loads(line)\n",
    "        dic[i] = {}\n",
    "        for key,value in review_data.items():\n",
    "            dic[i][key] = value\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = load_data(PATH['train'])\n",
    "dev_data = load_data(PATH['dev'])\n",
    "difficult_data = {}\n",
    "instances = 0\n",
    "with open(PATH['difficult']) as f:   \n",
    "    for line in f:\n",
    "        difficult_data[instances] = json.loads(line)\n",
    "        instances+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_encode(sent):\n",
    "    '''\n",
    "    Helper function to encode sentiment\n",
    "    ------\n",
    "    Takes in string description\n",
    "        'sent' - either positive or negative\n",
    "    Returns binary encoding\n",
    "        1 = positive sentiment\n",
    "        0 = negative sentiment\n",
    "    '''\n",
    "    if sent == 'positive':\n",
    "        return 1\n",
    "    if sent == 'negative':\n",
    "        return 0 \n",
    "    return 'unknown sentiment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(data):\n",
    "    '''\n",
    "    Function to clean the data\n",
    "    -----\n",
    "    Takes in data set from load_data()\n",
    "        'data' - nested dictionary  \n",
    "    Returns two lists\n",
    "        cleaned - X list\n",
    "        ys - y list\n",
    "    '''\n",
    "    cleaned = [] \n",
    "    ys = []\n",
    "    for idx in data:\n",
    "        review = data[idx].get('reviewText', None) # some data does not have a review text\n",
    "        summary = data[idx].get('summary', None) # some data does not have a summary \n",
    "        \n",
    "         #combine summary and review\n",
    "        if review == None and summary == None:\n",
    "            continue\n",
    "        elif review == None:\n",
    "            text = summary\n",
    "        elif summary == None:\n",
    "            text = review\n",
    "        else:\n",
    "            text = summary + ' ' + review\n",
    "        text = text.lower() \n",
    "        sequence = word_tokenize(text)  # splits gotta into got ta\n",
    "        cleaned.append(sequence)\n",
    "\n",
    "        # encode sentiment\n",
    "        ys.append(sent_encode(data[idx]['sentiment']))\n",
    "\n",
    "    return cleaned, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_difficult(data):\n",
    "    cleaned = [] \n",
    "    ys = []\n",
    "    for idx in data:\n",
    "        review = data[idx].get('reviewText', None) # some data does not have a review text\n",
    "        summary = data[idx].get('summary', None) # some data does not have a summary \n",
    "        review = review.lower()\n",
    "        sequence = word_tokenize(review)  # splits gotta into got ta\n",
    "        cleaned.append(sequence)\n",
    "\n",
    "        # encode sentiment\n",
    "        ys.append(sent_encode(data[idx]['sentiment']))\n",
    "    return cleaned, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean, y_train = clean(train_data)\n",
    "dev_clean, y_dev = clean(dev_data)\n",
    "train_clean += dev_clean \n",
    "cleaned_difficult, _ = clean_difficult(difficult_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_corpus(dataset):\n",
    "    '''\n",
    "    Function computing vocabluary and corpus for a dataset\n",
    "    -----\n",
    "    Takes a cleaned dataset - list \n",
    "        dataset - X list \n",
    "    Returns\n",
    "        vocab - set of unique tokens in dataset\n",
    "        corpus - list of strings; sentences in dataset \n",
    "    '''\n",
    "    vocab = set()\n",
    "    corpus = []\n",
    "    for text in dataset:\n",
    "        sentence = ''\n",
    "        for token in text:\n",
    "            vocab.add(token)\n",
    "            if token in ['.','!','?',',']:\n",
    "                sentence += token \n",
    "            else:\n",
    "                sentence += ' ' + token \n",
    "        corpus.append(sentence.lstrip()) \n",
    "    return vocab, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocabulary, train_corpus = get_vocab_corpus(train_clean)\n",
    "difficult_vocabulary, difficult_corpus = get_vocab_corpus(cleaned_difficult)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bow(vocab, corp):\n",
    "    '''\n",
    "    Function returning sparse matrix of Term Frequency â€” Inverse Document Frequencies\n",
    "    -----\n",
    "    Takes vocab and corpus, working with two lists\n",
    "        vocab - set of unique words\n",
    "        corpus - list of strings\n",
    "    Returns bag of words\n",
    "        bow - 2d matrix; input to model\n",
    "    '''\n",
    "    vocab = list(vocab) \n",
    "    vectorizer = TfidfVectorizer(vocabulary= vocab)\n",
    "    bow = vectorizer.fit_transform(corp) \n",
    "    return bow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bow = get_bow(train_vocabulary,train_corpus)\n",
    "difficult_bow = get_bow(train_vocabulary, difficult_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load model \n",
    "with open('../models/logreg_music.pkl','rb') as f:\n",
    "    log_reg_clf = pickle.load(f)\n",
    "y_pred = log_reg_clf.predict(difficult_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_test(test, ys):\n",
    "    '''\n",
    "    Function to insert predicitons into test data\n",
    "    '''\n",
    "    index = 0\n",
    "    for key in test:\n",
    "        test[key]['sentiment'] = reverse_encode(ys[index])\n",
    "        index += 1\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_encode(sent):\n",
    "    if sent == 1:\n",
    "        return 'positive'\n",
    "    if sent == 0:\n",
    "        return 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finished_test_data = pred_test(difficult_data,y_pred)\n",
    "finished_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('../data/difficult_cases/predictions.json', 'w') as f:\n",
    " #   json.dump(finished_test_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
