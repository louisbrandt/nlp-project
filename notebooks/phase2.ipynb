{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Generation of Difficult Cases\n",
    "\n",
    "The goal of this phase is to generate difficult instances for the task of sentiment analysis. The requirements are slightly different for both task types (classification versus sequence labeling), pick the task that you build your baseline model for in phase 1.\n",
    "\n",
    "You should in both situations participate in assignment 3. In other words, you will either do assignment 1 and 3 or assignment 2 and 3.\n",
    "\n",
    "\n",
    "#### How to Generate the Samples\n",
    "There are three main methods to generate the samples:\n",
    "* You can use the Checklist paper code: https://github.com/marcotcr/checklist\n",
    "* You can write code yourself to generate the samples. You can make use of any method you prefer, including a POS-tagger, word embeddings and contextualized embeddings\n",
    "* You can generate samples manually\n",
    "\n",
    "For each of these strategies you should think of a variety of types of difficult cases (so that not the whole set contains of the same types of samples), like the categories in Table 1 in \"the Checklist paper\".\n",
    "\n",
    "Note that you have to shortly present your approach in week14 (before the project proposal, you will get 2 minutes for phase 2 and 5 for the project proposal)\n",
    "\n",
    "#### For Inspiration:\n",
    "* [Beyond Accuracy: Behavioral Testing of NLP Models with CheckList](https://www.aclweb.org/anthology/2020.acl-main.442.pdf)\n",
    "* [Towards Linguistically Generalizable NLP Systems: A Workshop and Shared Task](https://www.aclweb.org/anthology/W17-5401.pdf)\n",
    "* [Breaking NLP: Using Morphosyntax, Semantics, Pragmatics and World\n",
    "Knowledge to Fool Sentiment Analysis Systems](https://www.aclweb.org/anthology/W17-5405.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classification\n",
    "\n",
    "The formal requirements are:\n",
    "\n",
    "* 100-1000 utterances should be handed in on **LearnIt before 30-03 11:59AM**\n",
    "* Must be in the same format as the training data : one (json) dict per line, and per instance needs at least: \"reviewText\", \"sentiment\", and \"category\" key.\n",
    "* The \"category\" key indicates which type of alternation/difficulty you included.\n",
    "* The gold labels must be correct!\n",
    "\n",
    "Assuming you write a function that generates examples, writing the final file can be done like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def swap(sentiment):\n",
    "    if sentiment == 'positive':\n",
    "        return 'negative'\n",
    "    elif sentiment == 'negative':\n",
    "        return 'positive'\n",
    "\n",
    "def dataGenerator(inputSents):\n",
    "    outputSents = []\n",
    "    for instance in inputSents:\n",
    "        if 'great' in instance[0]:\n",
    "            outputSents.append({'reviewText': instance[0].replace('great', 'not great'), 'sentiment': swap(instance[1]), 'category': 'negation'})\n",
    "    return outputSents\n",
    "\n",
    "inputSents = [['this is a great album', 'positive']]\n",
    "\n",
    "outFile = open('group13.json', 'w')\n",
    "for instance in dataGenerator(inputSents):\n",
    "    # goldLabel is a string, either 'positive' or 'negative', text contains the review, and category \n",
    "    # indicates the type of alternation you did.\n",
    "    outFile.write(json.dumps(instance) + '\\n')\n",
    "outFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should check whether your final file is in the correct format with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too little instances(0), please generate more\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "inputPath = 'group13.json'\n",
    "\n",
    "for lineIdx, line in enumerate(open(inputPath)):\n",
    "    try:\n",
    "        data = json.loads(line)\n",
    "    except ValueError as e:\n",
    "        print('error, instance ' + str(lineIdx+1) + ' is not in valid json format')\n",
    "        continue\n",
    "    if 'reviewText' not in data:\n",
    "        print(\"error, instance \" + str(lineIdx+1) + ' does not contain key \"reviewText\"')\n",
    "        continue\n",
    "    if 'sentiment' not in data:\n",
    "        print(\"error, instance \" + str(lineIdx+1) + ' does not contain key \"sentiment\"')\n",
    "        continue\n",
    "    if data['sentiment'] not in ['positive', 'negative']:\n",
    "        print(\"error, instance \" + str(lineIdx+1) + ': sentiment is not positive/negative')\n",
    "        continue\n",
    "        \n",
    "if lineIdx+1 < 100:\n",
    "    print('Too little instances(' + str(lineIdx) + '), please generate more')\n",
    "if lineIdx+1 > 1000:\n",
    "    print('Too many instances(' + str(lineIdx) + '), please generate more')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sequence labeling\n",
    "\n",
    "The formal requirements are:\n",
    "\n",
    "* 50-1000 utterances should be handed in on **LearnIt before 30-03 11:59AM**\n",
    "* Must be in the same format as the training data: one word per line, and labels in the third column.\n",
    "* The gold labels must be correct! Note that if you edit the number of tokens, you will have to make sure the labels align.\n",
    "* Also note that not all of the categories of the papers listed above will be applicable, as some of them focus only on classification tasks.\n",
    "\n",
    "\n",
    "Assuming you write a function that generates examples based on paraphrasing (synonyms), writing the final file can be done like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonym(token):\n",
    "    # In practice this dictionary could/should be filled from wordnet/word embeddings etc.\n",
    "    synonyms = {'great': 'startling'}\n",
    "    if token in synonyms:\n",
    "        return synonyms[token]\n",
    "    return token\n",
    "\n",
    "def dataGenerator(inputSents):\n",
    "    outputSents = []\n",
    "    for sent in inputSents:\n",
    "        outputSents.append([[], []])\n",
    "        for token, label in zip(sent[0], sent[1]):\n",
    "            outputSents[-1][0].append(synonym(token))\n",
    "            outputSents[-1][1].append(label)\n",
    "    return outputSents\n",
    "\n",
    "inputSents = [[['this', 'is', 'a', 'great', 'album'], ['O', 'O', 'O', 'B-Positive', 'O']]]\n",
    "\n",
    "outFile = open('group13.conll', 'w')\n",
    "for text, labels in dataGenerator(inputSents):\n",
    "    for wordIdx in range(len(text)):\n",
    "        outFile.write(str(wordIdx + 1) + '\\t' + text[wordIdx] + '\\t' + labels[wordIdx] + '\\n')\n",
    "    outFile.write('\\n')\n",
    "outFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should check whether your final file is in the correct format with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too little instances(1), please generate more\n"
     ]
    }
   ],
   "source": [
    "inputPath = 'group13.conll'\n",
    "\n",
    "numSents = 0\n",
    "\n",
    "for lineIdx, line in enumerate(open(inputPath)):\n",
    "    if line[0] == '#':\n",
    "        continue\n",
    "    if len(line) < 2:\n",
    "        numSents += 1\n",
    "        continue\n",
    "    tok = line.strip().split('\\t')\n",
    "    if len(tok) < 3:\n",
    "        print(str(lineIdx) + ': Not all columns defined: ' + line)\n",
    "        exit(1)\n",
    "    if not tok[0].isdigit():\n",
    "        print(str(lineIdx) + ': Invalid word index found: ' + line)\n",
    "        exit(1)\n",
    "    if len(tok[1].strip()) == 0:\n",
    "        print(str(lineIdx) + ': Empty token: ' + line)\n",
    "        exit(1)        \n",
    "    if tok[2] not in ['B-Positive', 'I-Positive', 'O', 'B-Negative', 'I-Negative']:\n",
    "        print(str(lineIdx) + ': Label is invalid: ' + line)\n",
    "        exit(1)\n",
    "        \n",
    "if numSents+1 < 50:\n",
    "    print('Too little instances(' + str(numSents) + '), please generate more')\n",
    "if numSents > 1000:\n",
    "    print('Too many instances(' + str(numSents) + '), please generate more')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prediction\n",
    "06-04 11:59AM is the deadline for handing in the predictions of the baseline on the difficult cases of all the groups. The datafile will be made available as soon as possible after your hand-ins (we aim for 02-04), and all you have to do is re-run your baseline from phase 1. Note that some of the meta-information might not be available, so if your baseline relies on those you have to either retrain without these features, or predict without these features.\n",
    "\n",
    "The codalab link will appear here, and will be posted on slack when available"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
