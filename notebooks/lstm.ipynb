{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data../all_data.pickle','rb') as f:\n",
    "    data = pickle.load(f)\n",
    "with open('../data../mono_lang_embeddings.pickle','rb') as f:\n",
    "    embedding_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(data['en']['train']['padded']), torch.from_numpy(data['en']['train']['y']))\n",
    "test_data = TensorDataset(torch.from_numpy(data['en']['test']['padded']), torch.from_numpy(data['en']['test']['y']))\n",
    "\n",
    "X , y = shuffle(torch.from_numpy(data['en']['train']['padded']),torch.from_numpy(data['en']['train']['y']))\n",
    "X = X[:10000]\n",
    "y = y[:10000]\n",
    "train_data = TensorDataset(X,y)\n",
    "\n",
    "Xt , yt = shuffle(torch.from_numpy(data['en']['test']['padded']),torch.from_numpy(data['en']['test']['y']))\n",
    "Xt = X[:100]\n",
    "yt = y[:100]\n",
    "test_data = TensorDataset(Xt,yt)\n",
    "# dataloaders\n",
    "batch_size = 50 \n",
    "\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self,no_layers,vocab_size,hidden_dim,embedding_dim,output_dim,drop_prob=0.5):\n",
    "        super(LSTM,self).__init__()\n",
    " \n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    " \n",
    "        self.no_layers = no_layers\n",
    "        self.vocab_size = vocab_size\n",
    "    \n",
    "        # embedding\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_dict['en']['matrix'])\n",
    "        \n",
    "        #lstm\n",
    "        self.lstm = nn.LSTM(input_size = embedding_dim,     # 200\n",
    "                            hidden_size = self.hidden_dim,  # 250\n",
    "                            num_layers = no_layers,         # 1\n",
    "                            bidirectional = True,\n",
    "                            batch_first = True)\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "        # linear and sigmoid layer\n",
    "        self.fc = nn.Linear(self.hidden_dim, output_dim)    # 250,1\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x,hidden): # [indexes], (h0,c0)\n",
    "        \n",
    "        # get embeddings for input \n",
    "        x = self.embedding(x) # (batch_size, seq_len, emb_dim) = (2,200,200)\n",
    "\n",
    "        # forward pass of lstm                          # shapes:  lstm_out = (batch_size, seq_len, hidden_dim) = (2,6,250)\n",
    "        #                                                          hidden   = (h0,c0) = ((1,2,250), (1,2,250)) --> states for each node for each batch\n",
    "        lstm_out, hidden = self.lstm(x.float(), hidden) # tensor.float needed for weird double / float error\n",
    "\n",
    "        # lstm_out: 3D --> 2D\n",
    "        lstm_flat = lstm_out.contiguous().view(-1, self.hidden_dim) \n",
    "\n",
    "        # dropout layer\n",
    "        lstm_do = self.dropout(lstm_flat)\n",
    "\n",
    "        # lstm --> fc\n",
    "        fc_out = self.fc(lstm_do)\n",
    "\n",
    "        # sigmoid activation\n",
    "        sigm = self.sig(fc_out) \n",
    "\n",
    "        # format output into p vector \n",
    "        out = sigm.view(batch_size,-1)\n",
    "        p = out[:,-1]\n",
    "        \n",
    "        return p, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim = 1 #binary classification\n",
    "hidden_dim = 250 \n",
    "vocab_size = len(embedding_dict['en']['lookup']) + 1 # + 1 for padding \n",
    "emb_dim = 200\n",
    "no_layers = 1 \n",
    "\n",
    "model = LSTM(no_layers,vocab_size,hidden_dim,emb_dim,output_dim)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "lr = 0.0001\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # initialise lstm hidden states h = (h0,c0)\n",
    "    h = (torch.zeros(2, batch_size, hidden_dim), torch.zeros(2, batch_size, hidden_dim)) # 2 for bidirectional  \n",
    "    running_loss = 0\n",
    "\n",
    "    for batch, y in train_loader:\n",
    "\n",
    "        h = tuple([each.data for each in h]) # otherwise model will try to back propagate through all hidden states in the epoch\n",
    "\n",
    "        optimizer.zero_grad() # do not want gradients to accumulate\n",
    "\n",
    "        # forward propegation \n",
    "        p, h = model.forward(batch,h)\n",
    "\n",
    "        # compute loss\n",
    "        loss = criterion(p.squeeze(),y.float())\n",
    "        running_loss += loss\n",
    "\n",
    "        # calculate gradients & update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    losses.append(running_loss*(batch_size/len(train_data)))\n",
    "    # if epoch % 2== 0:\n",
    "    print(f'Loss for epoch {epoch} = {running_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(range(epochs),losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y,p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(n):\n",
    "    if n > 0.5:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_correct(probs, target):\n",
    "  predictions = torch.tensor([predict(n)for n in probs])\n",
    "  corrects = (predictions == target)\n",
    "  return corrects.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_c = 0\n",
    "    model.eval()\n",
    "    h = (torch.zeros(2, batch_size, hidden_dim), torch.zeros(2, batch_size, hidden_dim)) \n",
    "    with torch.no_grad():\n",
    "        for batch, y in iterator:\n",
    "            predictions, _ = model(batch,h)\n",
    "            loss = criterion(predictions, y.float())\n",
    "            c = n_correct(predictions.reshape(-1,1), y)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_c += c\n",
    "    return epoch_loss / len(iterator), (epoch_c / (len(iterator)*batch_size)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "test_loss, test_acc"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
